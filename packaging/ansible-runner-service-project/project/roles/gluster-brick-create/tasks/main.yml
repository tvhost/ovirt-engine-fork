---
# rc 1 = Device or resource busy
- name: Clean up filesystem signature
  ansible.builtin.command: wipefs -a {{ item }}
  with_items: "{{ disks | default([]) }}"
  when: wipefs == 'yes' and item is defined
  register: resp
  failed_when: resp.rc not in [0, 1]

# Set data alignment for JBODs, by default it is 256K. This set_fact is not
# needed if we can always assume 256K for JBOD, however we provide this extra
# variable to override it.
- name: Set PV data alignment for JBOD
  ansible.builtin.set_fact:
    pv_dataalign: "{{ gluster_infra_dalign | default('256K') }}"
  when: disktype | upper in ['NONE', 'RAID0']

# Set data alignment for RAID
# We need KiB: ensure to keep the trailing `K' in the pv_dataalign calculation.
- name: Set PV data alignment for RAID
  ansible.builtin.set_fact:
    pv_dataalign: "{{ diskcount | int * stripesize | int }}K"
  when: disktype | upper in ['RAID6', 'RAID10']

- name: Set VG physical extent size for RAID
  ansible.builtin.set_fact:
    vg_pesize: "{{ diskcount | int * stripesize | int }}K"
  when: disktype | upper in ['RAID6', 'RAID10']

# rc 3 = already exists in filesystem
- name: Create volume groups
  ansible.builtin.command: "vgcreate --dataalignment {{ pv_dataalign }} -s {{ vg_pesize | default(4) }} {{ vgname }} {{ disks | join(' ') }}"
  register: resp
  failed_when: resp.rc not in [0, 3]
  changed_when: resp.rc == 0

# Chunksize is calculated as follows for GlusterFS' optimal performance.
# RAID6:
#    full_stripe_size = stripe_unit_size * no_of_data_disks
#
# Where full_stripe_size should be between 1 MiB and 2 MiB. And chunksize is set
# to full_stripe_size
#
- name: Calculate chunksize for RAID6/RAID10
  ansible.builtin.set_fact:
    lv_chunksize: "{{ stripesize | int * diskcount | int }}K"
  when: disktype | upper in ['RAID6', 'RAID10']

# For JBOD the thin pool chunk size is set to 256 KiB.
- name: Set chunksize for JBOD
  ansible.builtin.set_fact:
    lv_chunksize: '256K'
  when: disktype | upper in ['NONE', 'RAID0']

# rc 5 = Logical Volume 'name' already exists in volume group.
- name: Create a LV thinpool
  ansible.builtin.command: >
    lvcreate -l 100%FREE --chunksize {{ lv_chunksize }} --poolmetadatasize {{ pool_metadatasize }} --zero n
    --type thin-pool --thinpool {{ lvname }}_pool {{ vgname }}
  register: resp
  failed_when: resp.rc not in [0, 5]
  changed_when: resp.rc == 0

# rc 5 = Logical Volume 'name' already exists in volume group.
- name: Create thin logical volume
  ansible.builtin.command: "lvcreate -T {{ vgname }}/{{ lvname }}_pool -V {{ size }} -n {{ lvname }}"
  register: resp
  failed_when: resp.rc not in [0, 5]
  changed_when: resp.rc == 0

- ansible.builtin.include_tasks: lvmcache.yml
  when: ssd is defined and ssd

# rc 1 = Filesystem already exists
- name: Create an xfs filesystem
  ansible.builtin.command: >
    mkfs.xfs -f -K -i size=512 -n size=8192
    {% if 'raid' in disktype %} -d sw={{ diskcount }},su={{ stripesize }}k {% endif %}
     /dev/{{ vgname }}/{{ lvname }}
  register: resp
  failed_when: resp.rc not in [0, 1]
  changed_when: resp.rc == 0

- name: Create the backend directory, skips if present
  ansible.builtin.file:
    path: "{{ mntpath }}"
    state: directory
    mode: 0755

- name: Mount the brick
  ansible.posix.mount:
    name: "{{ mntpath }}"
    src: "/dev/{{ vgname }}/{{ lvname }}"
    fstype: "{{ fstype }}"
    opts: "inode64,noatime,nodiratime"
    state: mounted

- name: Set SELinux labels on the bricks
  ansible.builtin.command: "chcon -t glusterd_brick_t {{ mntpath }}"
  register: resp
  changed_when: resp.rc == 0
